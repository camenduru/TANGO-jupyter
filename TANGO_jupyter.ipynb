{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/TANGO-jupyter/blob/main/TANGO_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone -b dev https://github.com/camenduru/TANGO-hf /content/TANGO\n",
        "\n",
        "!pip install omegaconf wget decord smplx igraph av\n",
        "!pip install git+https://github.com/elliottzheng/batch-face\n",
        "\n",
        "!apt install -y -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/emage/smplx_models/smplx/SMPLX_NEUTRAL_2020.npz -d /content/TANGO/emage/smplx_models/smplx -o SMPLX_NEUTRAL_2020.npz\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/emage/AESKConv_240_100.bin -d /content/TANGO/emage -o AESKConv_240_100.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/emage/mean_vel_smplxflame_30.npy -d /content/TANGO/emage -o mean_vel_smplxflame_30.npy\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/Wav2Lip/checkpoints/mobilenet.pth -d /content/TANGO/Wav2Lip/checkpoints -o mobilenet.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/Wav2Lip/checkpoints/resnet50.pth -d /content/TANGO/Wav2Lip/checkpoints -o resnet50.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/Wav2Lip/checkpoints/wav2lip_gan.pth -d /content/TANGO/Wav2Lip/checkpoints -o wav2lip_gan.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/frame-interpolation-pytorch/film_net_fp16.pt -d /content/TANGO/frame-interpolation-pytorch -o film_net_fp16.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/frame-interpolation-pytorch/film_net_fp32.pt -d /content/TANGO/frame-interpolation-pytorch -o film_net_fp32.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_ckpts/ckpt.pth -d /content/TANGO/datasets/cached_ckpts -o ckpt.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_graph/youtube_test/speaker1.pkl -d /content/TANGO/datasets/cached_graph/youtube_test -o speaker1.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_graph/youtube_test/speaker7.pkl -d /content/TANGO/datasets/cached_graph/youtube_test -o speaker7.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_graph/youtube_test/speaker8.pkl -d /content/TANGO/datasets/cached_graph/youtube_test -o speaker8.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_graph/youtube_test/speaker9.pkl -d /content/TANGO/datasets/cached_graph/youtube_test -o speaker9.pkl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/data_json/youtube_test/speaker1.json -d /content/TANGO/datasets/data_json/youtube_test -o speaker1.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/data_json/youtube_test/speaker7.json -d /content/TANGO/datasets/data_json/youtube_test -o speaker7.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/data_json/youtube_test/speaker8.json -d /content/TANGO/datasets/data_json/youtube_test -o speaker8.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/data_json/youtube_test/speaker9.json -d /content/TANGO/datasets/data_json/youtube_test -o speaker9.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/example_female_voice_9_seconds.wav -d /content/TANGO/datasets/cached_audio -o example_female_voice_9_seconds.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/example_male_voice_9_seconds.wav -d /content/TANGO/datasets/cached_audio -o example_male_voice_9_seconds.wav\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/1wrQ6Msp7wM_00-00-39.69_00-00-45.68.mp4 -d /content/TANGO/datasets/cached_audio -o 1wrQ6Msp7wM_00-00-39.69_00-00-45.68.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/speaker8_jjRWaMCWs44_00-00-30.16_00-00-33.32.mp4 -d /content/TANGO/datasets/cached_audio -o speaker8_jjRWaMCWs44_00-00-30.16_00-00-33.32.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/speaker7_iuYlGRnC7J8_00-00-0.00_00-00-3.25.mp4 -d /content/TANGO/datasets/cached_audio -o speaker7_iuYlGRnC7J8_00-00-0.00_00-00-3.25.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4 -d /content/TANGO/datasets/cached_audio -o speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M  https://huggingface.co/spaces/H-Liu1997/TANGO/resolve/main/datasets/cached_audio/101099-00_18_09-00_18_19.mp4 -d /content/TANGO/datasets/cached_audio -o 101099-00_18_09-00_18_19.mp4\n",
        "\n",
        "%cd /content/TANGO\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import soundfile as sf\n",
        "import shutil\n",
        "import argparse\n",
        "from moviepy.tools import verbose_print\n",
        "from omegaconf import OmegaConf\n",
        "import random\n",
        "import numpy as np\n",
        "import json \n",
        "import librosa\n",
        "import emage.mertic\n",
        "from datetime import datetime\n",
        "from decord import VideoReader\n",
        "from PIL import Image\n",
        "import copy\n",
        "\n",
        "import importlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from tqdm import tqdm\n",
        "import smplx\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, ImageSequenceClip\n",
        "import igraph\n",
        "\n",
        "# import emage\n",
        "import utils.rotation_conversions as rc\n",
        "from utils.video_io import save_videos_from_pil\n",
        "from utils.genextend_inference_utils import adjust_statistics_to_match_reference\n",
        "from create_graph import path_visualization, graph_pruning, get_motion_reps_tensor, path_visualization_v2\n",
        "\n",
        "def search_path_dp(graph, audio_low_np, audio_high_np, loop_penalty=0.1, top_k=1, search_mode=\"both\", continue_penalty=0.1):\n",
        "    T = audio_low_np.shape[0]  # Total time steps\n",
        "    N = len(graph.vs)          # Total number of nodes in the graph\n",
        "\n",
        "    # Initialize DP tables\n",
        "    min_cost = [{} for _ in range(T)]  # min_cost[t][node_index] = list of tuples: (cost, prev_node_index, prev_tuple_index, non_continue_count, visited_nodes)\n",
        "\n",
        "    # Initialize the first time step\n",
        "    start_nodes = [v for v in graph.vs if v['previous'] is None or v['previous'] == -1]\n",
        "    for node in start_nodes:\n",
        "        node_index = node.index\n",
        "        motion_low = node['motion_low']      # Shape: [C]\n",
        "        motion_high = node['motion_high']    # Shape: [C]\n",
        "\n",
        "        # Cost using cosine similarity\n",
        "        if search_mode == \"both\":\n",
        "            cost = 2 - (np.dot(audio_low_np[0], motion_low.T) + np.dot(audio_high_np[0], motion_high.T))\n",
        "        elif search_mode == \"high_level\":\n",
        "            cost = 1 - np.dot(audio_high_np[0], motion_high.T)\n",
        "        elif search_mode == \"low_level\":\n",
        "            cost = 1 - np.dot(audio_low_np[0], motion_low.T)\n",
        "\n",
        "        visited_nodes = {node_index: 1}  # Initialize visit count as a dictionary\n",
        "\n",
        "        min_cost[0][node_index] = [ (cost, None, None, 0, visited_nodes) ]  # Initialize with no predecessor and 0 non-continue count\n",
        "\n",
        "    # DP over time steps\n",
        "    for t in range(1, T):\n",
        "        for node in graph.vs:\n",
        "            node_index = node.index\n",
        "            candidates = []\n",
        "\n",
        "            # Incoming edges to the current node\n",
        "            incoming_edges = graph.es.select(_to=node_index)\n",
        "            for edge in incoming_edges:\n",
        "                prev_node_index = edge.source\n",
        "                edge_id = edge.index\n",
        "                is_continue_edge = graph.es[edge_id]['is_continue']\n",
        "                prev_node = graph.vs[prev_node_index]\n",
        "                if prev_node_index in min_cost[t-1]:\n",
        "                    for tuple_index, (prev_cost, _, _, prev_non_continue_count, prev_visited) in enumerate(min_cost[t-1][prev_node_index]):\n",
        "                        # Loop punishment\n",
        "                        if node_index in prev_visited:\n",
        "                            loop_time = prev_visited[node_index]  # Get the count of previous visits\n",
        "                            loop_cost = prev_cost + loop_penalty * np.exp(loop_time)  # Apply exponential penalty\n",
        "                            new_visited = prev_visited.copy()\n",
        "                            new_visited[node_index] = loop_time + 1  # Increment visit count\n",
        "                        else:\n",
        "                            loop_cost = prev_cost\n",
        "                            new_visited = prev_visited.copy()\n",
        "                            new_visited[node_index] = 1  # Initialize visit count for the new node\n",
        "\n",
        "                        motion_low = node['motion_low']      # Shape: [C]\n",
        "                        motion_high = node['motion_high']    # Shape: [C]\n",
        "\n",
        "                        if search_mode == \"both\":\n",
        "                            cost_increment = 2 - (np.dot(audio_low_np[t], motion_low.T) + np.dot(audio_high_np[t], motion_high.T))\n",
        "                        elif search_mode == \"high_level\":\n",
        "                            cost_increment = 1 - np.dot(audio_high_np[t], motion_high.T)\n",
        "                        elif search_mode == \"low_level\":\n",
        "                            cost_increment = 1 - np.dot(audio_low_np[t], motion_low.T)\n",
        "\n",
        "                        # Check if the edge is \"is_continue\"\n",
        "                        if not is_continue_edge:\n",
        "                            non_continue_count = prev_non_continue_count + 1  # Increment the count of non-continue edges\n",
        "                        else:\n",
        "                            non_continue_count = prev_non_continue_count\n",
        "\n",
        "                        # Apply the penalty based on the square of the number of non-continuous edges\n",
        "                        continue_penalty_cost = continue_penalty * non_continue_count\n",
        "\n",
        "                        total_cost = loop_cost + cost_increment + continue_penalty_cost\n",
        "\n",
        "                        candidates.append( (total_cost, prev_node_index, tuple_index, non_continue_count, new_visited) )\n",
        "\n",
        "            # Keep the top k candidates\n",
        "            if candidates:\n",
        "                # Sort candidates by total_cost\n",
        "                candidates.sort(key=lambda x: x[0])\n",
        "                # Keep top k\n",
        "                min_cost[t][node_index] = candidates[:top_k]\n",
        "            else:\n",
        "                # No candidates, do nothing\n",
        "                pass\n",
        "\n",
        "    # Collect all possible end paths at time T-1\n",
        "    end_candidates = []\n",
        "    for node_index, tuples in min_cost[T-1].items():\n",
        "        for tuple_index, (cost, _, _, _, _) in enumerate(tuples):\n",
        "            end_candidates.append( (cost, node_index, tuple_index) )\n",
        "\n",
        "    if not end_candidates:\n",
        "        print(\"No valid path found.\")\n",
        "        return [], []\n",
        "\n",
        "    # Sort end candidates by cost\n",
        "    end_candidates.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Keep top k paths\n",
        "    top_k_paths_info = end_candidates[:top_k]\n",
        "\n",
        "    # Reconstruct the paths\n",
        "    optimal_paths = []\n",
        "    is_continue_lists = []\n",
        "    for final_cost, node_index, tuple_index in top_k_paths_info:\n",
        "        optimal_path_indices = []\n",
        "        current_node_index = node_index\n",
        "        current_tuple_index = tuple_index\n",
        "        for t in range(T-1, -1, -1):\n",
        "            optimal_path_indices.append(current_node_index)\n",
        "            tuple_data = min_cost[t][current_node_index][current_tuple_index]\n",
        "            _, prev_node_index, prev_tuple_index, _, _ = tuple_data\n",
        "            current_node_index = prev_node_index\n",
        "            current_tuple_index = prev_tuple_index\n",
        "            if current_node_index is None:\n",
        "                break  # Reached the start node\n",
        "        optimal_path_indices = optimal_path_indices[::-1]  # Reverse to get correct order\n",
        "        optimal_path = [graph.vs[idx] for idx in optimal_path_indices]\n",
        "        optimal_paths.append(optimal_path)\n",
        "\n",
        "        # Extract continuity information\n",
        "        is_continue = []\n",
        "        for i in range(len(optimal_path) - 1):\n",
        "            edge_id = graph.get_eid(optimal_path[i].index, optimal_path[i + 1].index)\n",
        "            is_cont = graph.es[edge_id]['is_continue']\n",
        "            is_continue.append(is_cont)\n",
        "        is_continue_lists.append(is_continue)\n",
        "\n",
        "    print(\"Top {} Paths:\".format(len(optimal_paths)))\n",
        "    for i, path in enumerate(optimal_paths):\n",
        "        path_indices = [node.index for node in path]\n",
        "        print(\"Path {}: Cost: {}, Nodes: {}\".format(i+1, top_k_paths_info[i][0], path_indices))\n",
        "\n",
        "    return optimal_paths, is_continue_lists\n",
        "\n",
        "\n",
        "def test_fn(model, device, iteration, candidate_json_path, test_path, cfg, audio_path, **kwargs):\n",
        "    torch.set_grad_enabled(False)\n",
        "    pool_path = candidate_json_path.replace(\"data_json\", \"cached_graph\").replace(\".json\", \".pkl\")\n",
        "    print(pool_path)\n",
        "    graph = igraph.Graph.Read_Pickle(fname=pool_path)\n",
        "    # print(len(graph.vs))\n",
        "\n",
        "    save_dir = os.path.join(test_path, f\"retrieved_motions_{iteration}\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    actual_model = model.module if isinstance(model, torch.nn.parallel.DistributedDataParallel) else model\n",
        "    actual_model.eval()\n",
        "\n",
        "    # with open(candidate_json_path, 'r') as f:\n",
        "    #     candidate_data = json.load(f)\n",
        "    all_motions = {}\n",
        "    for i, node in enumerate(graph.vs):\n",
        "        if all_motions.get(node[\"name\"]) is None:\n",
        "            all_motions[node[\"name\"]] = [node[\"axis_angle\"].reshape(-1)]\n",
        "        else:\n",
        "            all_motions[node[\"name\"]].append(node[\"axis_angle\"].reshape(-1))\n",
        "    for k, v in all_motions.items():\n",
        "        all_motions[k] = np.stack(v) # T, J*3\n",
        "        # print(k, all_motions[k].shape)\n",
        "    \n",
        "    window_size = cfg.data.pose_length\n",
        "    motion_high_all = []\n",
        "    motion_low_all = []\n",
        "    for k, v in all_motions.items():\n",
        "        motion_tensor = torch.from_numpy(v).float().to(device).unsqueeze(0)\n",
        "        _, t, _ = motion_tensor.shape\n",
        "        \n",
        "        if t >= window_size:\n",
        "            num_chunks = t // window_size\n",
        "            motion_high_list = []\n",
        "            motion_low_list = []\n",
        "\n",
        "            for i in range(num_chunks):\n",
        "                start_idx = i * window_size\n",
        "                end_idx = start_idx + window_size\n",
        "                motion_slice = motion_tensor[:, start_idx:end_idx, :]\n",
        "                \n",
        "                motion_features = actual_model.get_motion_features(motion_slice)\n",
        "                \n",
        "                motion_low = motion_features[\"motion_low\"].cpu().numpy()\n",
        "                motion_high = motion_features[\"motion_cls\"].unsqueeze(0).repeat(1, motion_low.shape[1], 1).cpu().numpy()\n",
        "\n",
        "                motion_high_list.append(motion_high[0])\n",
        "                motion_low_list.append(motion_low[0])\n",
        "\n",
        "            remain_length = t % window_size\n",
        "            if remain_length > 0:\n",
        "                start_idx = t - window_size\n",
        "                motion_slice = motion_tensor[:, start_idx:, :]\n",
        "\n",
        "                motion_features = actual_model.get_motion_features(motion_slice)\n",
        "                # motion_high = motion_features[\"motion_high_weight\"].cpu().numpy()\n",
        "                motion_low = motion_features[\"motion_low\"].cpu().numpy()\n",
        "                motion_high = motion_features[\"motion_cls\"].unsqueeze(0).repeat(1, motion_low.shape[1], 1).cpu().numpy()\n",
        "\n",
        "                motion_high_list.append(motion_high[0][-remain_length:])\n",
        "                motion_low_list.append(motion_low[0][-remain_length:])\n",
        "\n",
        "            motion_high_all.append(np.concatenate(motion_high_list, axis=0))\n",
        "            motion_low_all.append(np.concatenate(motion_low_list, axis=0))\n",
        "\n",
        "        else: # t < window_size:\n",
        "            gap = window_size - t\n",
        "            motion_slice = torch.cat([motion_tensor, torch.zeros((motion_tensor.shape[0], gap, motion_tensor.shape[2])).to(motion_tensor.device)], 1)\n",
        "            motion_features = actual_model.get_motion_features(motion_slice)\n",
        "            # motion_high = motion_features[\"motion_high_weight\"].cpu().numpy()\n",
        "            motion_low = motion_features[\"motion_low\"].cpu().numpy()\n",
        "            motion_high = motion_features[\"motion_cls\"].unsqueeze(0).repeat(1, motion_low.shape[1], 1).cpu().numpy()\n",
        "\n",
        "            motion_high_all.append(motion_high[0][:t])\n",
        "            motion_low_all.append(motion_low[0][:t])\n",
        "            \n",
        "    motion_high_all = np.concatenate(motion_high_all, axis=0)\n",
        "    motion_low_all = np.concatenate(motion_low_all, axis=0)\n",
        "    # print(motion_high_all.shape, motion_low_all.shape, len(graph.vs))\n",
        "    motion_low_all = motion_low_all / np.linalg.norm(motion_low_all, axis=1, keepdims=True)\n",
        "    motion_high_all = motion_high_all / np.linalg.norm(motion_high_all, axis=1, keepdims=True)\n",
        "    assert motion_high_all.shape[0] == len(graph.vs)\n",
        "    assert motion_low_all.shape[0] == len(graph.vs)\n",
        "    \n",
        "    for i, node in enumerate(graph.vs):\n",
        "        node[\"motion_high\"] = motion_high_all[i]\n",
        "        node[\"motion_low\"] = motion_low_all[i]\n",
        "\n",
        "    graph = graph_pruning(graph)\n",
        "    # for gradio, use a subgraph\n",
        "    if len(graph.vs) > 1800:\n",
        "        gap = len(graph.vs) - 1800\n",
        "        start_d = random.randint(0, 1800)\n",
        "        graph.delete_vertices(range(start_d, start_d + gap))\n",
        "    ascc_2 = graph.clusters(mode=\"STRONG\")\n",
        "    graph = ascc_2.giant()\n",
        "\n",
        "    # drop the id of gt\n",
        "    idx = 0\n",
        "    audio_waveform, sr = librosa.load(audio_path)\n",
        "    audio_waveform = librosa.resample(audio_waveform, orig_sr=sr, target_sr=cfg.data.audio_sr)\n",
        "    audio_tensor = torch.from_numpy(audio_waveform).float().to(device).unsqueeze(0)\n",
        "    \n",
        "    target_length = audio_tensor.shape[1] // cfg.data.audio_sr * 30\n",
        "    window_size = int(cfg.data.audio_sr * (cfg.data.pose_length / 30))\n",
        "    _, t = audio_tensor.shape\n",
        "    audio_low_list = []\n",
        "    audio_high_list = []\n",
        "\n",
        "    if t >= window_size:\n",
        "        num_chunks = t // window_size\n",
        "        # print(num_chunks, t % window_size)\n",
        "        for i in range(num_chunks):\n",
        "            start_idx = i * window_size\n",
        "            end_idx = start_idx + window_size\n",
        "            # print(start_idx, end_idx, window_size)\n",
        "            audio_slice = audio_tensor[:, start_idx:end_idx]\n",
        "\n",
        "            model_out_candidates = actual_model.get_audio_features(audio_slice)\n",
        "            audio_low = model_out_candidates[\"audio_low\"]\n",
        "            # audio_high = model_out_candidates[\"audio_high_weight\"]\n",
        "            audio_high = model_out_candidates[\"audio_cls\"].unsqueeze(0).repeat(1, audio_low.shape[1], 1)\n",
        "            # print(audio_low.shape, audio_high.shape)\n",
        "\n",
        "            audio_low = F.normalize(audio_low, dim=2)[0].cpu().numpy()\n",
        "            audio_high = F.normalize(audio_high, dim=2)[0].cpu().numpy()\n",
        "\n",
        "            audio_low_list.append(audio_low)\n",
        "            audio_high_list.append(audio_high)\n",
        "            # print(audio_low.shape, audio_high.shape)\n",
        "            \n",
        "\n",
        "        remain_length = t % window_size\n",
        "        if remain_length > 1:\n",
        "            start_idx = t - window_size\n",
        "            audio_slice = audio_tensor[:, start_idx:]\n",
        "\n",
        "            model_out_candidates = actual_model.get_audio_features(audio_slice)\n",
        "            audio_low = model_out_candidates[\"audio_low\"]\n",
        "            # audio_high = model_out_candidates[\"audio_high_weight\"]\n",
        "            audio_high = model_out_candidates[\"audio_cls\"].unsqueeze(0).repeat(1, audio_low.shape[1], 1)\n",
        "            \n",
        "            gap = target_length - np.concatenate(audio_low_list, axis=0).shape[1]\n",
        "            audio_low = F.normalize(audio_low, dim=2)[0][-gap:].cpu().numpy()\n",
        "            audio_high = F.normalize(audio_high, dim=2)[0][-gap:].cpu().numpy()\n",
        "            \n",
        "            # print(audio_low.shape, audio_high.shape)\n",
        "            audio_low_list.append(audio_low)\n",
        "            audio_high_list.append(audio_high)\n",
        "    else:\n",
        "        gap = window_size - t\n",
        "        audio_slice = audio_tensor \n",
        "        model_out_candidates = actual_model.get_audio_features(audio_slice)\n",
        "        audio_low = model_out_candidates[\"audio_low\"]\n",
        "        # audio_high = model_out_candidates[\"audio_high_weight\"]\n",
        "        audio_high = model_out_candidates[\"audio_cls\"].unsqueeze(0).repeat(1, audio_low.shape[1], 1)\n",
        "            \n",
        "        gap = target_length - np.concatenate(audio_low_list, axis=0).shape[1]\n",
        "        audio_low = F.normalize(audio_low, dim=2)[0][:gap].cpu().numpy()\n",
        "        audio_high = F.normalize(audio_high, dim=2)[0][:gap].cpu().numpy()\n",
        "        audio_low_list.append(audio_low)\n",
        "        audio_high_list.append(audio_high)\n",
        "    \n",
        "    audio_low_all = np.concatenate(audio_low_list, axis=0)\n",
        "    audio_high_all = np.concatenate(audio_high_list, axis=0)\n",
        "    path_list, is_continue_list = search_path_dp(graph, audio_low_all, audio_high_all, top_k=1, search_mode=\"both\")\n",
        "    \n",
        "    res_motion = []\n",
        "    counter = 0\n",
        "    for path, is_continue in zip(path_list, is_continue_list):\n",
        "        # print(path)\n",
        "        # res_motion_current = path_visualization(\n",
        "        #   graph, path, is_continue, os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\"), audio_path=audio_path, return_motion=True, verbose_continue=True\n",
        "        # )\n",
        "        res_motion_current = path_visualization_v2(\n",
        "          graph, path, is_continue, os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\"), audio_path=audio_path, return_motion=True, verbose_continue=True\n",
        "        )\n",
        "\n",
        "        video_temp_path = os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\")\n",
        "        \n",
        "        video_reader = VideoReader(video_temp_path)\n",
        "        video_np = []\n",
        "        for i in range(len(video_reader)):\n",
        "            if i == 0: continue\n",
        "            video_frame = video_reader[i].asnumpy()\n",
        "            video_np.append(Image.fromarray(video_frame))\n",
        "        adjusted_video_pil = adjust_statistics_to_match_reference([video_np])\n",
        "        save_videos_from_pil(adjusted_video_pil[0], os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\"), fps=30, bitrate=2000000)\n",
        "\n",
        "\n",
        "        audio_temp_path = audio_path\n",
        "        lipsync_output_path = os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\")\n",
        "        checkpoint_path = './Wav2Lip/checkpoints/wav2lip_gan.pth'  # Update this path to your Wav2Lip checkpoint\n",
        "        os.system(f'python ./Wav2Lip/inference.py --checkpoint_path {checkpoint_path} --face {video_temp_path} --audio {audio_temp_path} --outfile {lipsync_output_path} --nosmooth')\n",
        "\n",
        "        res_motion.append(res_motion_current)\n",
        "        np.savez(os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.npz\"), motion=res_motion_current)\n",
        "    \n",
        "        start_node = path[1].index\n",
        "        end_node = start_node + 100\n",
        "    print(f\"delete gt-nodes {start_node}, {end_node}\")\n",
        "    nodes_to_delete = list(range(start_node, end_node))\n",
        "    graph.delete_vertices(nodes_to_delete)\n",
        "    graph = graph_pruning(graph)\n",
        "    path_list, is_continue_list = search_path_dp(graph, audio_low_all, audio_high_all, top_k=1, search_mode=\"both\")\n",
        "    res_motion = []\n",
        "    counter = 1\n",
        "    for path, is_continue in zip(path_list, is_continue_list):\n",
        "        res_motion_current = path_visualization(\n",
        "          graph, path, is_continue, os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\"), audio_path=audio_path, return_motion=True, verbose_continue=True\n",
        "        )\n",
        "        video_temp_path = os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\")\n",
        "        \n",
        "        video_reader = VideoReader(video_temp_path)\n",
        "        video_np = []\n",
        "        for i in range(len(video_reader)):\n",
        "            if i == 0: continue\n",
        "            video_frame = video_reader[i].asnumpy()\n",
        "            video_np.append(Image.fromarray(video_frame))\n",
        "        adjusted_video_pil = adjust_statistics_to_match_reference([video_np])\n",
        "        save_videos_from_pil(adjusted_video_pil[0], os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\"), fps=30, bitrate=2000000)\n",
        "\n",
        "\n",
        "        audio_temp_path = audio_path\n",
        "        lipsync_output_path = os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.mp4\")\n",
        "        checkpoint_path = './Wav2Lip/checkpoints/wav2lip_gan.pth'  # Update this path to your Wav2Lip checkpoint\n",
        "        os.system(f'python ./Wav2Lip/inference.py --checkpoint_path {checkpoint_path} --face {video_temp_path} --audio {audio_temp_path} --outfile {lipsync_output_path} --nosmooth')\n",
        "        res_motion.append(res_motion_current)\n",
        "        np.savez(os.path.join(save_dir, f\"audio_{idx}_retri_{counter}.npz\"), motion=res_motion_current)\n",
        "    \n",
        "    result = [\n",
        "        os.path.join(save_dir, f\"audio_{idx}_retri_0.mp4\"),\n",
        "        os.path.join(save_dir, f\"audio_{idx}_retri_1.mp4\"),\n",
        "        os.path.join(save_dir, f\"audio_{idx}_retri_0.npz\"),\n",
        "        os.path.join(save_dir, f\"audio_{idx}_retri_1.npz\")\n",
        "    ]\n",
        "    return result\n",
        "\n",
        "\n",
        "def init_class(module_name, class_name, config, **kwargs):\n",
        "    module = importlib.import_module(module_name)\n",
        "    model_class = getattr(module, class_name)\n",
        "    instance = model_class(config, **kwargs)\n",
        "    return instance\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def prepare_all(yaml_name):\n",
        "    if yaml_name.endswith(\".yaml\"):\n",
        "        config = OmegaConf.load(yaml_name)\n",
        "        config.exp_name = os.path.basename(yaml_name)[:-5]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported config file format. Only .yaml files are allowed.\")\n",
        "    save_dir = os.path.join(config.output_dir, config.exp_name)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    return config\n",
        "\n",
        "def save_first_10_seconds(video_path, output_path=\"./save_video.mp4\"):\n",
        "    import cv2\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        return\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frames_to_save = fps * 10\n",
        "    frame_count = 0\n",
        "    \n",
        "    while cap.isOpened() and frame_count < frames_to_save:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        out.write(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "\n",
        "character_name_to_yaml = {\n",
        "  \"speaker8_jjRWaMCWs44_00-00-30.16_00-00-33.32.mp4\": \"./datasets/data_json/youtube_test/speaker8.json\",\n",
        "  \"speaker7_iuYlGRnC7J8_00-00-0.00_00-00-3.25.mp4\": \"./datasets/data_json/youtube_test/speaker7.json\",\n",
        "  \"speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4\": \"./datasets/data_json/youtube_test/speaker9.json\",\n",
        "  \"1wrQ6Msp7wM_00-00-39.69_00-00-45.68.mp4\": \"./datasets/data_json/youtube_test/speaker1.json\",\n",
        "  \"101099-00_18_09-00_18_19.mp4\": \"./datasets/data_json/show_oliver_test/Stupid_Watergate_-_Last_Week_Tonight_with_John_Oliver_HBO-FVFdsl29s_Q.mkv.json\",\n",
        "}\n",
        "\n",
        "def tango(audio_path, character_name, seed, create_graph=False, video_folder_path=None):\n",
        "    cfg = prepare_all(\"./configs/gradio.yaml\")\n",
        "    cfg.seed = seed\n",
        "    seed_everything(cfg.seed)\n",
        "    experiment_ckpt_dir = experiment_log_dir = os.path.join(cfg.output_dir, cfg.exp_name)\n",
        "    saved_audio_path = \"./saved_audio.wav\"\n",
        "\n",
        "    # Load audio from the specified path\n",
        "    audio_waveform, sample_rate = librosa.load(audio_path, sr=None)  # Load with original sample rate\n",
        "    sf.write(saved_audio_path, audio_waveform, sample_rate)\n",
        "\n",
        "    # Resample the audio to 16000 Hz\n",
        "    resampled_audio = librosa.resample(audio_waveform, orig_sr=sample_rate, target_sr=16000)\n",
        "    required_length = int(16000 * (128 / 30)) * 2\n",
        "    resampled_audio = resampled_audio[:required_length]\n",
        "    sf.write(saved_audio_path, resampled_audio, 16000)\n",
        "    audio_path = saved_audio_path  # Update the audio_path to point to the saved resampled audio\n",
        "\n",
        "    yaml_name = character_name_to_yaml.get(character_name.split(\"/\")[-1], \"./datasets/data_json/youtube_test/speaker1.json\")\n",
        "    cfg.data.test_meta_paths = yaml_name\n",
        "    print(yaml_name, character_name.split(\"/\")[-1])\n",
        "\n",
        "    if character_name.split(\"/\")[-1] not in character_name_to_yaml.keys():\n",
        "        create_graph = True\n",
        "        os.makedirs(\"./outputs/tmpvideo/\", exist_ok=True)\n",
        "        save_first_10_seconds(character_name, \"./outputs/tmpvideo/save_video.mp4\")\n",
        "\n",
        "    if create_graph:\n",
        "        video_folder_path = \"./outputs/tmpvideo/\"\n",
        "        data_save_path = \"./outputs/tmpdata/\"\n",
        "        json_save_path = \"./outputs/save_video.json\"\n",
        "        graph_save_path = \"./outputs/save_video.pkl\"\n",
        "        os.system(f\"cd ./SMPLer-X/ && python app.py --video_folder_path {video_folder_path} --data_save_path {data_save_path} --json_save_path {json_save_path} && cd ..\")\n",
        "        os.system(f\"python ./create_graph.py --json_save_path {json_save_path} --graph_save_path {graph_save_path}\") \n",
        "        cfg.data.test_meta_paths = json_save_path\n",
        "\n",
        "    smplx_model = smplx.create(\n",
        "        \"./emage/smplx_models/\", \n",
        "        model_type='smplx',\n",
        "        gender='NEUTRAL_2020', \n",
        "        use_face_contour=False,\n",
        "        num_betas=300,\n",
        "        num_expression_coeffs=100, \n",
        "        ext='npz',\n",
        "        use_pca=False,\n",
        "    )\n",
        "    model = init_class(cfg.model.name_pyfile, cfg.model.class_name, cfg)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    model.smplx_model = smplx_model\n",
        "    model.get_motion_reps = get_motion_reps_tensor\n",
        "    \n",
        "    local_rank = 0  \n",
        "    torch.cuda.set_device(local_rank)\n",
        "    device = torch.device(\"cuda\", local_rank)\n",
        "\n",
        "    smplx_model = smplx_model.to(device).eval()\n",
        "    model = model.to(device)\n",
        "    model.smplx_model = model.smplx_model.to(device)\n",
        "\n",
        "    checkpoint_path = \"./datasets/cached_ckpts/ckpt.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "    model.load_state_dict(new_state_dict, strict=False)\n",
        "    \n",
        "    test_path = os.path.join(experiment_ckpt_dir, f\"test_{0}\")\n",
        "    os.makedirs(test_path, exist_ok=True)\n",
        "    result = test_fn(model, device, 0, cfg.data.test_meta_paths, test_path, cfg, audio_path)\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/TANGO\n",
        "seed = 42\n",
        "wav = \"./datasets/cached_audio/example_male_voice_9_seconds.wav\"\n",
        "mp4 = \"./datasets/cached_audio/speaker9_o7Ik1OB4TaE_00-00-38.15_00-00-42.33.mp4\"\n",
        "video_output_1, video_output_2, file_output_1, file_output_2 = tango(wav, mp4, seed)\n",
        "\n",
        "from IPython.display import Video\n",
        "# Video(video_output_1, embed=True)\n",
        "Video(video_output_2, embed=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
